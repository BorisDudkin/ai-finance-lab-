# Copilot / AI agent instructions for ai-finance-lab

Purpose: Give succinct, project-specific guidance so an AI coding agent can be productive immediately.

## Quick start ‚úÖ
- Run small scripts directly: `python 01_macro_regime_classifier/classify.py` (uses `sample_inputs/fomc.txt`).
- Typical pattern: import functions and call them from tests or higher-level scripts (no full app server here).

## Key files & folders (what matters) üîß
- `01_macro_regime_classifier/classify.py` ‚Äî LLM-based classification. Uses `call_llm(messages, temperature=0.0)`.
- `02_asset_allocation_signals/signals.py` ‚Äî Generates allocation JSON. Note system prompt: **"Return valid JSON only."** and `json.loads(call_llm(...))`.
- `02_asset_allocation_signals/schema.json` ‚Äî intended schema for signals (currently empty; update when changing signal shape).
- `03_generative_ai/` ‚Äî evaluation, RAG artifacts and structured outputs (mostly scaffolding).
- `04_macro_research_rag/query/answer.py` ‚Äî RAG answering pattern: `answer(question, context_chunks)` concatenates context and asks LLM: **"Answer only from context."**
- `docs/risk_controls.md` and `docs/model_limitations.md` ‚Äî project guardrails (human-in-the-loop, confidence thresholds, provenance, conservative outputs).
- `05_multi_agent_systems/` and `06_agents/` ‚Äî higher-level agent/assistant scaffolds used for multi-agent experimentation.

## Project-specific LLM conventions (follow these precisely) üìú
- call signature: `call_llm(messages, temperature=...)` ‚Äî the repository expects a small wrapper (`llm_client`) that returns raw text. Keep this interface.
- Deterministic outputs: use `temperature=0.0` for tasks that must be machine-parseable (classification, JSON signals).
- Use `system` messages to constrain behavior (examples: "Be conservative and precise.", "Return valid JSON only.", "Answer only from context.").
- When the task expects machine-readable output, embed a clear JSON schema in the user message and parse with `json.loads()`; always wrap parsing with try/except and raise a clear, testable error if parsing fails.
- Include an explicit `confidence` field for graded outputs (0-1) ‚Äî `signals.py` expects this.

## RAG and provenance patterns üîç
- RAG query code concatenates text chunks and sends them in a prompt constrained by a system message. Agents should avoid hallucinations by respecting the system message that restricts answers to provided context.
- Preserve source attribution when adding RAG functionality (store chunk ids / filenames alongside chunk text if you add ingestion code).

## Implementation notes & best practices specific to this codebase üí°
- `llm_client` is intentionally external to the repo ‚Äî when adding it, implement these behaviors:
  - Accept `messages` and `temperature` parameters and return the LLM text response.
  - Read credentials via environment variables (do **not** commit secrets).
  - Implement retry/backoff and basic logging; caller code assumes synchronous call-and-parse semantics.
- When expecting JSON from the model, prefer schema-first prompts (see `signals.py`) and validate with `jsonschema` if available.
- Use `sample_inputs/` for small fixtures (e.g., `01_macro_regime_classifier/sample_inputs`) when writing tests.
- There are currently no test/CI files in the repo ‚Äî when adding functionality, create small unit tests that mock `call_llm` responses (pytests) to ensure deterministic behavior.

## What to avoid ‚ùå
- Don't change prompt text lightly ‚Äî small wording changes can change model behavior. If you do change prompts, add tests that assert expected parseable outputs.
- Do not commit API keys or sensitive configuration.

## When you change anything here ‚úçÔ∏è
- Update `02_asset_allocation_signals/schema.json` when you change signal shape.
- Add/adjust unit tests that mock `llm_client.call_llm` and verify parsing, schema validity, and confidence ranges.

---

If anything is missing or unclear, say which behavior or file you'd like expanded and I will iterate. Thank you!